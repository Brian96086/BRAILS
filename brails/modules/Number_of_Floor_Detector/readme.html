<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.16: http://docutils.sourceforge.net/" />
<title>Number of Floor Detector</title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 7952 2016-07-26 18:15:59Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

.subscript {
  vertical-align: sub;
  font-size: smaller }

.superscript {
  vertical-align: super;
  font-size: smaller }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type="image/svg+xml"], object[type="application/x-shockwave-flash"] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left, table.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right, table.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

table.align-center {
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

.align-top    {
  vertical-align: top }

.align-middle {
  vertical-align: middle }

.align-bottom {
  vertical-align: bottom }

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* "booktabs" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document" id="number-of-floor-detector">
<h1 class="title">Number of Floor Detector</h1>

<div class="section" id="purpose-of-the-module">
<h1>Purpose of the Module</h1>
<p>This module enables automated detection of number of floors in a building from image input.</p>
<div class="figure">
<img alt="Sample model floor detections" src="images/sampleModelOutputs.gif" style="width: 448.0px; height: 383.59999999999997px;" />
<p class="caption">Figure 1: Sample floor detections of the pretrained model provided with this module, shown by bright green bounding boxes. The percentage value shown on the top right corner of each bounding box indicates the model's confidence level associated with that prediction.</p>
</div>
<div class="section" id="copyright">
<h2>Copyright</h2>
<pre class="literal-block">
Copyright (c) 2020, The Regents of the University of California
Contact: Barbaros Cetiner at bacetiner&#64;ucla.edu
</pre>
</div>
<div class="section" id="bsd-3-caluse-license">
<h2>BSD 3-Caluse license</h2>
<pre class="literal-block">
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

* Neither the name of the copyright holder nor the names of its
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS &quot;AS IS&quot;
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
</pre>
</div>
</div>
<div class="section" id="installation">
<h1>Installation</h1>
<p>The following commands clone the BRAILS repository and install the number of floor detection module. Requirements are installed using pip and weights of all used models are downloaded. Make sure to run the last line to add the current folder to the PYTHONPATH variable to avoid issues in training.</p>
<pre class="literal-block">
git clone https://github.com/NHERI-SimCenter/BRAILS.git BRAILS
cd BRAILS/brails/modules/NumberOfFloorDetection
python3 -m pip install -r requirements.txt
export PYTHONPATH=$PYTHONPATH:`pwd`
</pre>
</div>
<div class="section" id="program">
<h1>Program</h1>
<div class="section" id="input-data-format-for-training-and-testing">
<h2>Input Data Format for Training and Testing</h2>
<p>Training, validation, and test folders should be separate. All three folders must be stored in the COCO format and follow the convention defined below. For training a model using a custom dataset, training, validation, and annotations folders must exist. Bounding box annotations for the training and validation folders shall be placed under the annotations folder. The current version of the module only takes horizontal bounding box input.</p>
<pre class="literal-block">
IMG_FOLDER
â”œâ”€â”€ train
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ .......... (and so on)
â”œâ”€â”€ valid
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ .......... (and so on)
â”œâ”€â”€ test
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ .......... (and so on)
â””â”€â”€ annotations
    â”œâ”€â”€ instances_train.json
    â”œâ”€â”€ instances_valid.json
    â””â”€â”€ classes.txt
</pre>
</div>
<div class="section" id="running-the-module-using-the-pretrained-floor-detection-model">
<h2>Running the Module Using the Pretrained Floor Detection Model</h2>
<p>The module is bundled with a pretrained floor detection model, trained on 80,000 training samples. This model can be called out-of-the-box via <a class="reference external" href="infer.py">infer.py</a>., a powerful post-processor custom-tailored to convert bounding box detections to floor counts. The basic syntax to perform inferences on a set of images requires defining the path for the images and the type of computational environment (i.e., use of CPU or GPU units for inference) by the user as follows.</p>
<pre class="literal-block">
python3 infer.py
    --im_path &quot;/path/to/images&quot;
    --gpu_enabled True
</pre>
<p>Using the command line option <tt class="docutils literal"><span class="pre">--model_path</span></tt>, <tt class="docutils literal">infer.py</tt> can be called with a custom model trained by the user. For a brief description of all the options built into <tt class="docutils literal">infer.py</tt>, please use the <tt class="docutils literal">infer.py <span class="pre">--help</span></tt> syntax. Below is a complete list of these options.</p>
<pre class="literal-block">
--im_path (default: &quot;VOC/test/&quot;) Path for the building images that will be inferred by module

--model_path (default: &quot;models/efficientdet-d4_trained.pth&quot;) Path for the pretrained inference model.
                                                             Do NOT define this argument if the pretrained model bundled with the module will be used

--gpu_enabled (default: True) Enable GPU processing (Enter False for CPU-based inference)

--csv_out (default: &quot;nFloorPredict.csv&quot;) Name of the CSV output file where the inference results will be written
</pre>
</div>
<div class="section" id="model-training">
<h2>Model Training</h2>
<p>If the user wishes to further train the pretrained floor detection model that is bundled with this module, or train a separate model by finetuning an EfficientDet model already trained on COCO 2017 detection
datasets, using custom data; the folder structure shown in <a class="reference internal" href="#input-data-format-for-training-and-testing">Input Data Format for Training and Testing</a> shall be strictly followed. Model training is performed using <a class="reference external" href="train.py">train.py</a>.</p>
<p>Following is an comprehensive list of the available command line parameters. The user may also use the <tt class="docutils literal">train.py <span class="pre">--help</span></tt> syntax to view a brief version of the list below.</p>
<pre class="literal-block">
-c (default: 4) Compund coefficient for the EfficientDet backbone, e.g., enter 7 for EfficientDet-D7

-n (default: 0) Number of loader processes to use with Pytorch DataLoader

--top_only (default: False) True if desired to finetune the regressor and the classifier (head) only.
                            False if desired to finetune the entire network

--num_gpus (default: 1) Number of GPUs available for training. Enter 0 for CPU-based training

--optim (default: &quot;adamw&quot;) Optimizer used for training. Available options: AdamW and SGD.
                           Use of AdamW until the last stage of training then switching to SGD recommended

--lr (default: 0.0001) Optimizer learning rate

--batch_size (default: 2) The number of images used per training step

--num_epochs (default: 25) Number of training epochs

--data_path (default: &quot;datasets/&quot;) Path for the root folder of dataset

--val_interval (default: 1) Number of epoches between model validating. Enter 1 for validating at the end of each epoch

--save_interval (default: 5) Number of epoches between model saving. Enter 1 for saving at the end of each epoch

--es_min_delta (default: 0.0) Early stopping parameter: Minimum change in loss to qualify as an improvement

--es_patience (default: 0) Number of epochs with no improvement after which training will be stopped.
                           Set to 0 to disable early stopping

--customModel_path (default: &quot;models/efficientdet-d4_trained.pth&quot;) Path for the custom pretrained model desired to be used in training.
                           This option is meant for continued training of an existing model.
                           It can be used for models trained on an EfficientDet backbone only
</pre>
<p>For example, the command to train a floor detection model <strong>on CPU</strong> by <strong>fine-tuning the full EfficientDet-D4 backbone trained on COCO dataset</strong> for <strong>25 epochs</strong> using a <strong>learning rate of 0.0001</strong>:</p>
<pre class="literal-block">
python3 train.py
    --num_gpus 0
    --head_only False
    --num_epochs 25
    --lr 0.0001
</pre>
</div>
</div>
<div class="section" id="pretrained-model">
<h1>Pretrained Model</h1>
<div class="section" id="model-architecture">
<h2>Model Architecture</h2>
<p>In general, all modern object detectors can be said to consist of three main components:</p>
<ol class="arabic simple">
<li>A backbone network that extracts features from the given image at different scales,</li>
<li>A feature network that receives multiple levels of features from the backbone and returns a list of fused features that identify the dominant features of the image,</li>
<li>A class and box network that takes the fused features as input to predict the class and location of each object, respectively.</li>
</ol>
<p>EfficientDet models use EfficientNets pretrained on ImageNet for their backbone network. For the feature network, EfficienDet models use a novel bidirectional feature pyramid network (BiFPN), which takes level 3 through 7 features from the backbone network and repeatedly fuses these features in top-down and bottom-up directions. Both BiFPN layers and class/box layers are repeated multiple times with the number of repetations depending on the compund coefficient of the architecture. Figure 2 provides and overview of the described structure. For further details please see the seminal work by <a class="reference external" href="https://arxiv.org/abs/1911.09070">Tan, Pang, and Le</a>.</p>
<div class="figure">
<img alt="Model architecture" id="modelarch" src="images/EfficientDetArchitecture.PNG" style="width: 817.0px; height: 294.5px;" />
<p class="caption">Figure 2: A high-level representation of the EfficientDet architecture</p>
</div>
<p>Remarkable performance gains can be attained in image classification by jointly scaling up all dimensions of neural network width, depth, and input resolution, as noted in the study by <a class="reference external" href="https://arxiv.org/abs/1905.11946">Tan and Le</a>. Inspired by this work, EfficienDet utilizes a new compound scaling method for object detection that jointly increases all dimensions of the backbone network, BiFPN, class/box network, and input image resolution, using a simple compound coefficient, Ï†. A total of 8 compounding levels are defined for EffcienDet, i.e., Ï† = 0 to 8, with EfficientDet-D0 being the simplest and EfficientDet-D8 being the most complex of the network architectures.</p>
<p>As shown in Figure 3, at the time this work was published, EfficientDet object detection algorithms attained the state-of-the-art performance on the COCO dataset. Also suggested in Figure 3 is the more complex the network architecture is, the higher the detection performance will be. From a practical standpoint, however, architecture selection will depend on the availability of computational resources. For example, to train a model on an architecture with a compound coefficient higher than 4, a GPU with a memory of more than 11 GB will almost always be required.</p>
<div class="figure">
<img alt="Detection performance" id="detperf" src="images/EfficientDetPerfComp.PNG" style="width: 397.20000000000005px; height: 313.6px;" />
<p class="caption">Figure 3: A comparison of the performance and accuracy levels of EfficienDet models over other popular object detection architectures on the COCO dataset</p>
</div>
</div>
<div class="section" id="model-validation">
<h2>Model Validation</h2>
<p>On a randomly selected set of in-the-wild building images from New Jersey's Bergen, Middlesex, and Moris Counties, the model attains an F1-score of 86%. Here, in-the-wild building images are defined as street-level photos that may contain multiple buildings and are captured with random camera properties. Figure 4 is the confusion matrix of the model inferences on the aforementioned in-the-wild test set.</p>
<div class="figure">
<img alt="Confusion matrix (in-the-wild dataset)" src="images/confusionMatrixWild.png" style="width: 503.99999999999994px; height: 352.79999999999995px;" />
<p class="caption">Figure 4: Confusion matrix of the pretrained model on the in-the-wild test set</p>
</div>
<p>If the test images are constrained such that a single building exists in each image and the images are captured such that the image plane is nearly parallel to the frontal plane of the building facade, the F1-score of the model is determined as 94.7%. Figure 5 shows the confusion matrix for the pretrained model on a test set generated according to these constraints.</p>
<div class="figure">
<img alt="Confusion matrix (clean dataset)" src="images/confusionMatrixClean.png" style="width: 503.99999999999994px; height: 352.79999999999995px;" />
<p class="caption">Figure 5: Confusion matrix of the pretrained model on the dataset containing lightly distorted/obstructed images of individual buildings</p>
</div>
</div>
</div>
</div>
</body>
</html>
